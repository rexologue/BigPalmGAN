Скрипт для Fine-Tuning'а BigGAN модели

Пример запуска:

```bash
python fine_tune.py \
--epochs количество_эпох \
--num_classes количество_классов \
--lr_g скорость_обучение_генератора \
--lr_d скорость_обучение_дискриминатора \
--weight_decay_g параметр_L2_регуляризации_для_весов_генератора \
--weight_decay_d параметр_L2_регуляризации_для_весов_дискриминатора \
--betas_g beta1_для_генератора beta2_для_генератора \
--betas_d beta1_для_дискриминатора beta2_для_дискриминатора \
--batch_size размер_батча \
--g_loss_weight вес_adversarial_loss_генератора \
--pixel_loss_weight вес_pixel_loss_генератора \
--perceptual_loss_weight вес_perceptual_loss_генератора \
--accumulation_steps количество_шагов_для_gradient_accumulation \
--lambda_gp коэффициент_штрафа_градиентов \
--unfreeze_last_n сколько_слоев_генератора_разморозить_(всего_16_установите_-1_чтобы_разморозить_все) \
--use_augs использовать_ли_аугментации \
--train_dir /Путь/до/обучающих/данных \
--eval_dir /Путь/до/валидационных/данных \
--print_every_n_batches как_часто_выводить_метрики \
--output_dir /Путь/сохранения/весов/и/сэмплов/изображений \
--inception /Путь/до/inception/модели/вычисляющей/fid.pth
```

Также доступны следующие условные параметры:

1. --ckp /Путь/сохраненного/чекпоинта.pth
2. --only_g_ckp /Путь/сохраненного/чекпоинта/только/генератора.pth

Используйте *1*, чтобы продолжить прерванное обучение.

Используйте *2*, чтобы начать обучение только если у вас есть веса предобученного генератора.

Не используйте ни один из них, если хотите начать обучение с нуля. 

**ЗАМЕЧАНИЕ:** Чекпоинты сохраняются в конце каждой эпохи, занимая около 700 Мб на диске
